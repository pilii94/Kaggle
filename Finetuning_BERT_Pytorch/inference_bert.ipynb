{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import spacy\n","import glob\n","import os\n","import yaml\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import StratifiedKFold\n","from tqdm import tqdm\n","from tqdm import tqdm\n","from pathlib import Path\n","import random\n","import numpy as np\n","import pickle\n","from termcolor import colored\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, TFBertModel\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.metrics import mean_squared_error\n","\n","!pip install GPUtil\n","\n","import torch\n","from GPUtil import showUtilization as gpu_usage\n","from numba import cuda\n","\n","def free_gpu_cache():\n","    print(\"Initial GPU Usage\")\n","    gpu_usage()                             \n","\n","    torch.cuda.empty_cache()\n","\n","    cuda.select_device(0)\n","    cuda.close()\n","    cuda.select_device(0)\n","\n","    print(\"GPU Usage after emptying the cache\")\n","    gpu_usage()\n","\n","free_gpu_cache()                \n","\n","if torch.cuda.is_available():        \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, please check.')\n","    # device = torch.device(\"cpu\")\n","\n","def evaluate_predictions(y_true, y_pred, model_name, logger, metric_funcs=None, out_path=None, prefix='train'):\n","    if metric_funcs is None:\n","        metric_funcs = {\n","            'Accuracy': {\n","                'fpointer': accuracy_score,\n","                'args': {}\n","            },\n","            'F1-score': {\n","                'fpointer': f1_score,\n","                'args': {\n","                    'average': 'macro'\n","                }\n","            }\n","        }\n","\n","    metrics = {}\n","    for i in range(len(classes)):\n","        idx = (y_true == i)\n","        y_i = np.array(y_true == i, dtype=int)\n","        preds_i = np.array(y_pred == i, dtype=int)\n","\n","        print_str = f'{convert_indexes2classes([i])[0]} '\n","        for k in metric_funcs.keys():\n","            metric_val = metric_funcs[k]['fpointer'](y_i, preds_i, **metric_funcs[k]['args'])\n","            print_str += f'{k}: ' + colored(f'{metric_val:.4f}', 'green') + ' '\n","\n","            metrics[f'{k} ({classes[i]})'] = float(metric_val)\n","\n","        logger.info(print_str)\n","\n","    print_str = colored(f'{model_name} ', attrs=['bold'])\n","    for k in metric_funcs.keys():\n","        metric_val = metric_funcs[k]['fpointer'](y_true, y_pred, **metric_funcs[k]['args'])\n","        print_str += colored(f'{k}: ' + colored(f'{metric_val:.4f}', 'red') + ' ', attrs=['bold'])\n","        metrics[f'{k}'] = float(metric_val)\n","    logger.info(print_str)\n","\n","    if out_path is not None:\n","        with open(os.path.join(out_path, f'{prefix}_results.yaml'), 'w') as file:\n","            documents = yaml.dump(metrics, file)\n","\n","    # Confusion Matrix\n","    short_labels = ['NS', 'S ', 'HS']\n","    m = confusion_matrix(y_true, y_pred)\n","    conf_matrix = pd.DataFrame(m, columns=short_labels, index=short_labels)\n","    conf_matrix.columns = pd.MultiIndex.from_product([['Predicted'], conf_matrix.columns])\n","\n","    logger.info(f'Confusion matrix:\\n{conf_matrix}')\n","\n","    return metrics\n","\n","class BertClassifier():\n","    def __init__(self, logger=None):\n","        self.logger = logger\n","        self.seed_val = 42\n","        self.model = None\n","        self.is_trained = False\n","        self.model_name = \"bert-base-uncased\"\n","        self.tokenizer = BertTokenizerFast.from_pretrained(self.model_name, do_lower_case=True)\n","        self.MAX_LEN = 500 \n","        self.lr = 2e-5\n","        self.eps = 1e-8 \n","        self.batch_size = 5 #bigger batch size we get out of memory\n","        self.epochs = 1\n","        \n","        \n","    def process_sentences(self, X):\n","        #Sents to ids, padding and truncating\n","        input_ids = []\n","        for sent in X:\n","            encoded_sent = self.tokenizer.encode(sent, add_special_tokens = True)\n","            input_ids.append(encoded_sent)\n","        input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=self.MAX_LEN, dtype=\"long\", \n","                            value=0, truncating=\"pre\", padding=\"pre\")#pre, post\n","        return input_ids\n","        \n","    def create_attentionmasks(self, input_ids):\n","        # Create attention masks\n","        attention_masks = []\n","        for sent in input_ids:\n","            att_mask = [int(token_id > 0) for token_id in sent]\n","            attention_masks.append(att_mask)\n","        return attention_masks\n","\n","    def print_rmse(self, preds, labels):\n","#         pred_flat = np.argmax(preds, axis=1).flatten()\n","#         labels_flat = labels.flatten()\n","\n","        return  np.sqrt(mean_squared_error(labels, preds))\n","    \n","\n","\n","        \n","    def fit(self, X,y):\n","        \"\"\"\n","        This function trains a model using pretrained Bert\n","        \"\"\"\n","        input_ids = self.process_sentences(X)\n","  \n","        attention_masks = self.create_attentionmasks(input_ids)\n","        # Use 90% for training and 10% for validation.\n","        train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y, \n","                                                                    random_state=2018, test_size=0.1)\n","        # Do the same for the masks.\n","        train_masks, validation_masks, _, _ = train_test_split(attention_masks, y,\n","                                                    random_state=2018, test_size=0.1)\n","        # Convert into torch \n","        train_inputs = torch.tensor(train_inputs)\n","        validation_inputs = torch.tensor(validation_inputs)\n","        \n","        train_masks = torch.tensor(train_masks)\n","        validation_masks = torch.tensor(validation_masks)\n","       \n","        train_labels = torch.tensor(train_labels.to_numpy(),dtype=torch.float)\n","        validation_labels = torch.tensor(validation_labels.to_numpy(),dtype=torch.float)\n","\n","         # Create the DataLoader tr.\n","        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","        train_sampler = RandomSampler(train_data)\n","        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size)\n","        # Create the DataLoader val.\n","        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","        validation_sampler = SequentialSampler(validation_data)\n","        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=self.batch_size)\n","\n","\n","\n","        basemodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                num_labels = 1, \n","                                                output_attentions = False, \n","                                                output_hidden_states = False)\n","        \n","        basemodel.cuda()\n","        # optimizer = torch.optim.Adam(basemodel.parameters(), \n","        #           lr = self.lr, \n","        #           eps = self.eps \n","        #         ) \n","\n","        # optimizer = torch.optim.SGD(basemodel.parameters(),\n","        #     lr = self.lr\n","        # ) \n","\n","        optimizer = AdamW(basemodel.parameters(), # Implements Adam algorithm with weight decay fix as introduced in Decoupled Weight Decay Regularization.\n","                  lr = self.lr, \n","                  eps = self.eps \n","                ) \n","\n","        total_steps = len(train_dataloader) * self.epochs\n","\n","        #Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer\n","        scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, \n","                                            num_training_steps = total_steps)\n","        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, verbose=True)\n","        \n","\n","        random.seed(self.seed_val)\n","        np.random.seed(self.seed_val)\n","        torch.manual_seed(self.seed_val)\n","        torch.cuda.manual_seed_all(self.seed_val)\n","        # Store the average loss\n","        loss_values = []\n","\n","        es = EarlyStopping(patience=50, logger = self.logger)\n","\n","        for epoch_i in range(0, self.epochs):\n","            \n","            self.logger.info(f'============= Epoch: {epoch_i + 1} / {self.epochs} =============')\n","            self.logger.info('Training')\n","            total_loss = 0\n","            basemodel.train()\n","            for step, batch in enumerate(train_dataloader):\n","                b_input_ids = batch[0].to(device)\n","                b_input_mask = batch[1].to(device)\n","                b_target = batch[2].to(device)\n","                basemodel.zero_grad() \n","\n","                outputs = basemodel(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_target)\n","                loss = outputs[0]\n","                total_loss += loss.item()\n","                loss.backward() #calc gradients\n","                torch.nn.utils.clip_grad_norm_(basemodel.parameters(), 1.0)\n","                optimizer.step()\n","                scheduler.step()\n","            avg_train_loss = total_loss / len(train_dataloader) \n","            loss_values.append(avg_train_loss)\n","            \n","            self.logger.info(f\"Avg training loss: {avg_train_loss}\")\n","            self.logger.info(\"Running Validation\")\n","            basemodel.eval()\n","            eval_rmse = 0\n","            nb_eval_steps, nb_eval_examples = 0, 0\n","            for batch in validation_dataloader:\n","                batch = tuple(t.to(device) for t in batch)\n","                b_input_ids, b_input_mask, b_target = batch\n","                with torch.no_grad():\n","                    outputs = basemodel(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","                logits = outputs[0]\n","                logits = logits.detach().cpu().numpy()\n","                label_ids = b_target.to('cpu').numpy()\n","\n","                tmp_eval_rmse= self.print_rmse(logits, label_ids)\n","                eval_rmse += tmp_eval_rmse\n","\n","                nb_eval_steps += 1\n","            # Report the final accuracy for this validation run.\n","            self.logger.info(f\"RMSE: {eval_rmse/nb_eval_steps}\")\n","          \n","            \n","            if es.step(eval_rmse, basemodel, self.tokenizer, epoch_i):\n","                self.logger.info(\"Early stopping.\")\n","                break  # early stop criterion is met, we can stop now \n","        \n","        self.logger.info(\"Training complete.\")\n","        self.logger.info(\"Retrieving best model.\")\n","        self.model, self.tokenizer = es.get_best_model()\n","        self.is_trained = True\n","\n","\n","\n","      \n","    def predict(self, X):\n","        assert self.is_trained, 'Model should be trained before inference.'\n","        input_ids = self.process_sentences(X)\n","        attention_masks = self.create_attentionmasks(input_ids)\n","        prediction_inputs = torch.tensor(input_ids)\n","        prediction_masks = torch.tensor(attention_masks)\n","       \n","        # DataLoader.\n","        prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n","        prediction_sampler = SequentialSampler(prediction_data)\n","        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=self.batch_size)\n","\n","        # evaluation mode\n","        self.model.eval()\n","        preds = []\n","        predictions = []\n","        final_preds = []#np.array([]) \n","        # Predict \n","        for batch in prediction_dataloader:\n","            \n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask = batch\n","            \n","            with torch.no_grad():\n","                output = self.model(b_input_ids, token_type_ids=None, \n","                                attention_mask=b_input_mask)\n","#             logits = outputs[0]\n","          \n","#             logits = logits.detach().cpu().numpy()\n","#             predictions.append(logits)\n","                output = output[\"logits\"].squeeze(-1)\n","                preds.append(output.cpu().numpy())\n","\n","        predictions = np.concatenate(preds)\n","        \n","        return predictions\n","#         for i in range(len(predictions)):\n","#             pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","#             final_preds = np.concatenate((final_preds, pred_labels_i), axis=0)\n","        \n","#         return  np.asarray(final_preds)\n","        \n","       \n","    def save(self, path):\n","        if self.is_trained:\n","            output_dir = Path(path)\n","            if not output_dir.exists():\n","                os.makedirs(output_dir)\n","            model_config = {}\n","            # serialize model \n","            with open(os.path.join(output_dir, f'model_config.yaml'), 'w') as file:\n","                documents = yaml.dump(model_config, file)\n","\n","            model_to_save = self.model.module if hasattr(self.model, 'module') else self.model  \n","            model_to_save.save_pretrained(str(output_dir))\n","            self.tokenizer.save_pretrained(str(output_dir))\n","\n","            self.logger.info(f'Saved model to {output_dir}')\n","        else:\n","            self.logger.warning('Cannot save the model. Train it first.')\n","\n","    def load(self, path):\n","        output_dir = Path(path)\n","        with open(os.path.join(output_dir, f'model_config.yaml')) as file:\n","            model_config = yaml.load(file, Loader=yaml.FullLoader)\n","        \n","        # Load trained model and vocabulary fine-tuned\n","        self.model = BertForSequenceClassification.from_pretrained(str(output_dir),num_labels=1)\n","        self.tokenizer = BertTokenizerFast.from_pretrained(str(output_dir))\n","\n","        # Copy the model to the GPU. Check if prediction is ok in CPU\n","        self.model.to(device)\n","       \n","        self.is_trained = True\n","\n","\n","class EarlyStopping(object):\n","    def __init__(self, logger = None, mode='min', min_delta=0, patience=10, percentage=False):\n","        self.mode = 'min'\n","        self.logger = logger\n","        self.best_model = None\n","        self.best_tokenizer = None\n","        self.min_delta = min_delta\n","        self.patience = patience\n","        self.best = None\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","        self._init_is_better(mode, min_delta, percentage)\n","\n","        if patience == 0:\n","            self.is_better = lambda a, b: True\n","            self.step = lambda a: False\n","\n","    def step(self, metrics, basemodel, tokenizer, epoch):\n","        if epoch == 0:\n","            self.best_model = basemodel\n","            self.best_tokenizer = tokenizer\n","\n","        if self.best is None:\n","            self.best = metrics\n","            return False\n","\n","        if self.is_better(metrics, self.best):\n","            self.num_bad_epochs = 0\n","            self.best = metrics\n","            self.logger.info(f'Creating model checkpoint')\n","            # self.model_checkpoint(basemodel, tokenizer)\n","            self.best_model = basemodel\n","            self.best_tokenizer = tokenizer\n","\n","        else:\n","            self.num_bad_epochs += 1\n","\n","        if self.num_bad_epochs >= self.patience:\n","            return True\n","\n","        return False\n","\n","    def _init_is_better(self, mode='min', min_delta=0, percentage=False):\n","        if mode not in {'min', 'max'}:\n","            raise ValueError('mode ' + mode + ' is unknown!')\n","        if not percentage:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - min_delta\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + min_delta\n","        else:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - (\n","                            best * min_delta / 100)\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + (\n","                            best * min_delta / 100)\n","    def get_best_model(self):\n","        return self.best_model, self.best_tokenizer\n","    \n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T08:38:48.471120Z","iopub.execute_input":"2021-05-31T08:38:48.471514Z","iopub.status.idle":"2021-05-31T08:38:48.484714Z","shell.execute_reply.started":"2021-05-31T08:38:48.471434Z","shell.execute_reply":"2021-05-31T08:38:48.483735Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"../input/commonlitreadabilityprize/sample_submission.csv\n../input/commonlitreadabilityprize/train.csv\n../input/commonlitreadabilityprize/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:38:50.081384Z","iopub.execute_input":"2021-05-31T08:38:50.081714Z","iopub.status.idle":"2021-05-31T08:39:10.193083Z","shell.execute_reply.started":"2021-05-31T08:38:50.081683Z","shell.execute_reply":"2021-05-31T08:39:10.191274Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7410 sha256=458ef21934f47b9bdc8fa08661bb82fa2742967ea20ceddf64b09620e88458af\n  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\nInitial GPU Usage\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  0% |\nGPU Usage after emptying the cache\n| ID | GPU | MEM |\n------------------\n|  0 | 10% |  2% |\nThere are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"source":["## Prediction for submission"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["\n","import pandas as pd\n","test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n","sample = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n","\n","model = BertClassifier(logger)\n","model.load()\n","preds = model.predict(test[\"excerpt\"])\n","sample[\"target\"] = preds\n","sample.to_csv(\"submission.csv\")"],"metadata":{"execution":{"iopub.status.busy":"2021-05-31T08:42:42.466022Z","iopub.execute_input":"2021-05-31T08:42:42.466402Z","iopub.status.idle":"2021-05-31T08:42:42.487367Z","shell.execute_reply.started":"2021-05-31T08:42:42.466363Z","shell.execute_reply":"2021-05-31T08:42:42.486617Z"},"trusted":true},"execution_count":6,"outputs":[]}]}